\chapter{Elementi distinti}

\begin{problem*}
    Progettare e analizzare un algoritmo di data streaming che permetta di
    approssimare il numero di elementi distinti.
\end{problem*}

Cominciamo fissando le notazioni usate nel seguito. Sia \(\chi = x_1 x_2
\dots x_n\) uno stream di \(n\) caratteri scelti da un insieme di cardinalit\`a
\(m\le n\) di simboli distinti. Denotiamo con \(D(\chi)\) il numero di elementi
distinti effettivamente presenti nello stream.

Per prima cosa risolviamo un problema pi\`u semplice: dato un numero \(t\)
vogliamo decidere con alta probabili\`a se \(D(\chi) \gg t\) oppure \(D(\chi) \ll
t\). Pi\`u precisamente vogliamo risolvere il seguente
\begin{problem*}
  Data una soglia \(t\) e uno stream \(\chi\), rispondere
  \begin{itemize}
    \item S\`i se \(D(\chi)\ge t\),
    \item No se \(D(\chi)<\frac{t}{2}\),
    \item indifferentemente S\`i o No altrimenti,
  \end{itemize}
  con probabilit\`a maggiore di \(1 - \delta\) di essere corretto.
\end{problem*}

Supponiamo di avere una funzione hash \(h:X\rightarrow [1,t]\) ideale, tale
cio\`e che \(\forall i\mbox{, Pr}[h(x)=i]=\frac{1}{t}\). In termini di
questa funzione siamo quasi in grado di risolvere il precedente problema.
Abbiamo infatti il seguente
\begin{algorithm}
  \caption{Contatore con rumore}
  \begin{algorithmic}[1]
    \Function{NoisyCounter}{h}
    \For {each \(x_i\in X\)}
      \If {\(h(x_i) = t\)}
        \State \Return {S\`i}
      \EndIf
    \EndFor
    \State \Return {No}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{lemma}
  Supponiamo che \(D(\chi)\ge t\) o \(D(\chi)<\frac{t}{2}\). Allora il precedente
  algoritmo \`e corretto con probabilit\`a maggiore di \(0.6\).
\end{lemma}
\begin{proof}
  Osserviamo che basta stimare la probabilit\`a di rispondere ``No''. Infatti
  questa sar\`a la probabilit\`a d'errore qualora \(D(\chi)\) fosse maggiore di
  \(t\) e la probabilit\`a di successo nell'altro caso.
  \begin{itemize}
    \item Supponiamo \(D(\chi)\ge t\). TODO
    \item Supponiamo invece \(D(\chi)<\frac{t}{2}\). TODO
  \end{itemize}
\end{proof}

Abbiamo quindi un modo per ottenere una risposta corretta in pi\`u del \(60\%\)
dei casi. Possiamo superare qualunque soglia di confidenza semplicemente
ripetendo abbastanza volte il precedente algoritmo e scegliendo il risultato
ottenuto pi\`u frequentemente. Sia dunque \(H = \left\{h_j\,|\,j\in[k]\right\}\) 
una famiglia di funzioni hash ideali e indipendenti.
\begin{algorithm}
  \caption{}
  \begin{algorithmic}[1]
    \State {yes\_votes = 0}
    \For {each \(h_j\in H\)}
      \If {\(\textsc{NoisyCounter}(h_j)\) = S\`i}
        \State {yes\_votes\,++}
      \EndIf
    \EndFor
    \If {yes\_votes \(>\frac{k}{2}\)}
      \State \Return {S\`i}
    \EndIf
    \State \Return {No}
  \end{algorithmic}
\end{algorithm}

\begin{lemma}
  TODO
\end{lemma}
\begin{proof}
  Sia \(Z_i\) l'indicatrice dell'evento \(\{\textsc{NoisyCounter}(h_j)=\mbox{S\`i}\}\).
  Il precedente lemma garantisce che \(\mbox{Pr}[Z_i]\ge 0.6\). Sia allora \(Z=
  \sum_{i=1}^k{Z_i}\). Osserviamo che dalla linearit\`a della speranza segue 
  immediatamente che \(\E[Z]\ge 0.6k\). Di conseguenza abbiamo:
  \[\mbox{Pr}\left[Z\le\frac{k}{2}\right]\le\mbox{Pr}\left[Z\le\frac{0.5}{0.6}
  \E[Z]\right]\mbox{.}\]
  Possiamo maggiorare il membro di destra con una delle disuguaglianze di
  Chernoff, cio\`e:
  \[\mbox{Pr}[Z\le(1-\xi)\E[Z]]\le\exp{\left(-\frac{\E[Z]\cdot\xi^2}{2}\right)}\mbox{.}\]
  Abbiamo dunque, ponendo \(1-\xi = \frac{0.5}{0.6}\), che:
  \[\mbox{Pr}\left[Z\le\frac{k}{2}\right]\le\exp{\left(-\frac{0.6 k\cdot{\frac{0.1}{0.6}}^2}{2}\right)}\mbox{,}\]
  perci\`o possiamo rendere arbitrariamente  piccola la probabilit\`a di rispondere
  ``No''. Di conseguenza, ragionando come nella dimostrazione del precedente lemma,
  possiamo rendere arbitrariamente vicina a \(1\) la probabilit\`a di successo.
\end{proof}
